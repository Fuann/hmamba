# -*- coding: utf-8 -*-
# @Time    : 04/15/24
# @Author  : Fu-An Chao
# @Affiliation  : National Taiwan Normal University
# @Email   : fuann@ntnu.edu.tw
# @File    : hmamba.py

import sys
import math
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.nn.init import xavier_uniform_
from functools import partial
from .modules.bimamba import Mamba as BiMamba
from mamba_ssm.ops.triton.layernorm import RMSNorm

class PredictionHead(nn.Module):
    def __init__(self, in_dim, out_dim, dropout=0):
        super(PredictionHead, self).__init__()
        self.dense = nn.Linear(in_dim, in_dim)
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(in_dim, out_dim)

    def forward(self, x):
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.linear(x)
        return x

class AttentionPooling(nn.Module):
    def __init__(self, in_dim, scale=1.0):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(in_dim, 1),
            nn.GELU(),
        )
        self.scale = scale if scale else in_dim ** -0.5

    def forward(self, x, score, mask):
        w = self.attention(score).float() / self.scale
        w[mask==0] = float('-inf')
        w = torch.softmax(w, 1) # B x 1 x 1
        x = torch.sum(w * x, dim=1) # B x D
        return x

class MeanPooling(nn.Module):
    def __init__(self, in_dim):
        super().__init__()

    def forward(self, x, mask):
        input_mask_expanded = (
            mask.unsqueeze(-1).expand(x.size()).float()
        )
        return torch.sum(x * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or self.head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, mask=None):
        B, N, C = x.shape

        #print(C)
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale

        # (B, T)
        if mask is not None:
            attn = attn.masked_fill(
                mask.unsqueeze(1).unsqueeze(2),
                -1e9,
            )

        attn = attn.softmax(dim=-1)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class Block(nn.Module):

    def __init__(self, dim, d_state=16, d_conv=2,  expand=4, mlp_ratio=4. , drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, block_type="bimamba"):
        super().__init__()
        # NOTE: attention
        self.norm1 = norm_layer(dim)
        if block_type=="bimamba":
            self.model = BiMamba(d_model=dim, d_state=d_state, d_conv=d_conv, expand=expand, bimamba_type="v2")
        elif block_type=="transformer":
            self.model = Attention(dim, num_heads=1, qkv_bias=False, qk_scale=None, attn_drop=0, proj_drop=0)
        else:
            raise ValueError(f"only 'bimamba', 'transformer' of block_type is avaliable.")

        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        x = x + self.drop_path(self.model(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class HMamba(nn.Module):
    def __init__(self, embed_dim=24, gop_dim=None, ssl_dim=None, raw_dim=None,
                    kernel_size=3, d_state=16, d_conv=2, expand=4, drop=0., feat_drop=0., max_len=50, vocab_size=81,
                    act_layer=nn.GELU, norm_layer=RMSNorm, use_bies=False, use_cano=True, use_pos=True, use_conv=False, pool_mode="mean", block_type="bimamba"):
        super().__init__()
        self.gop_dim = gop_dim
        self.ssl_dim = ssl_dim
        self.raw_dim = raw_dim
        self.embed_dim = embed_dim
        self.use_bies = use_bies
        self.use_cano = use_cano
        self.use_pos = use_pos
        self.use_conv = use_conv
        self.pool_mode = pool_mode
        self.max_len = max_len
        self.vocab_size = vocab_size

        """
        model
        """
        self.phn_block_1 = Block(dim=embed_dim, drop=drop, d_state=d_state, d_conv=d_conv, expand=expand, act_layer=act_layer, norm_layer=norm_layer, block_type=block_type)
        self.phn_block_2 = Block(dim=embed_dim, drop=drop, d_state=d_state, d_conv=d_conv, expand=expand, act_layer=act_layer, norm_layer=norm_layer, block_type=block_type)
        self.phn_block_3 = Block(dim=embed_dim, drop=drop, d_state=d_state, d_conv=d_conv, expand=expand, act_layer=act_layer, norm_layer=norm_layer, block_type=block_type)
        self.wrd_block_4 = Block(dim=embed_dim, drop=drop, d_state=d_state, d_conv=d_conv, expand=expand, act_layer=act_layer, norm_layer=norm_layer, block_type=block_type)
        self.utt_block_5 = Block(dim=embed_dim, drop=drop, d_state=d_state, d_conv=d_conv, expand=expand, act_layer=act_layer, norm_layer=norm_layer, block_type=block_type)
        

        """
        frontend
        """
        if gop_dim:
            self.frontend_dim = gop_dim
        else:
            self.frontend_dim = 0

        if isinstance(ssl_dim, list):
            for dim in ssl_dim:
                self.frontend_dim += dim
        else:
            self.frontend_dim = self.frontend_dim + ssl_dim if ssl_dim else self.frontend_dim

        if raw_dim:
            self.frontend_dim += raw_dim

        self.feat_drop = nn.Dropout(feat_drop)
        if self.frontend_dim != self.gop_dim:
            self.in_proj = nn.Linear(self.frontend_dim, embed_dim)

        """
        embedding
        """
        # position embedding, (50 seq-len)
        if self.use_pos:
            self.pos_embed = nn.Parameter(torch.zeros(1, self.max_len, embed_dim))
            trunc_normal_(self.pos_embed, std=.02)

        # NOTE: phone embedding, torch-emb (39 phns + 1 pad), 40
        if self.use_cano:
            self.phn_embed = nn.Embedding(self.vocab_size+1, embed_dim, padding_idx=0)
            self.canophn_proj = nn.Linear(embed_dim, embed_dim)

        # NOTE: bies embedding, 0:pad, 1:B, 2:E, 3:I, 4:S, 5:LS, 6:GS
        if self.use_bies:
            self.bies_embed = nn.Embedding(7, embed_dim, padding_idx=0)
            self.bies_proj = nn.Linear(embed_dim, embed_dim)

        """
        head
        """
        # realized phone recognition, (80 phns + 1 pad)
        self.phn_mlp_recog = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, self.vocab_size), nn.Dropout(0.1))

        # phone, 1=accuracy
        self.phn_mlp_score = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1))

        # word, 1=accuracy, 2=stress, 3=total
        if self.use_conv:
            self.wrd_conv = nn.Sequential(
                nn.Conv1d(
                    embed_dim,  2*embed_dim, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2, groups=embed_dim
                ),
                nn.Conv1d(
                    2*embed_dim, embed_dim, kernel_size=1, stride=1, padding=0
                ),
            )
        self.wrd_mlp_score_1 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #acc
        self.wrd_mlp_score_2 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #stress
        self.wrd_mlp_score_3 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #total

        if self.pool_mode == "attn":
            self.utt_pool = AttentionPooling(embed_dim)
        elif self.pool_mode == "score-attn":
            self.utt_pool = AttentionPooling(4)
        elif self.pool_mode == "mean":
            self.utt_pool = MeanPooling(embed_dim)
        else:
            raise ValueError(f"{self.pool_mode} is not available")

        self.utt_mlp_score_1 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #acc
        self.utt_mlp_score_2 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #comp
        self.utt_mlp_score_3 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #flu
        self.utt_mlp_score_4 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #pro
        self.utt_mlp_score_5 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Dropout(0.0), nn.Linear(embed_dim, 1)) #tot

    def mapping(self, x, slope=1.0, y_shift=1.0):
        # 1 / e^-(b0+b1x)
        b1 = torch.nn.Parameter(torch.FloatTensor([1.0]), requires_grad=True).to(x.device)
        b0 = torch.nn.Parameter(torch.FloatTensor([0.0]), requires_grad=True).to(x.device)
        return 1.0 / torch.exp(-b0-b1*x) + y_shift

    def forward(self, x, x2, x3, canophn, bies=None, mask=None):
        # x shape in [batch_size, sequence_len, feat_dim]
        # phn in [batch_size, seq_len]
        # mask in [batch_size, seq_len]

        # NOTE: ssl feats
        if isinstance(x2, list):
            for i, ssl in enumerate(x2):
                ssl = self.feat_drop(ssl)
                # NOTE: gop feats
                if self.gop_dim is None and i==0:
                   x = ssl
                   continue
                if isinstance(x, torch.Tensor):
                    x = torch.cat((x, ssl), 2)

        # NOTE: raw feats: dur, energy
        if isinstance(x3, torch.Tensor):
            dur = x3[:,:,0:1]                  # 0 = dur
            eng = x3[:,:,1:8]                  # 1-7 = eng

            # NOTE: concat gop+raw
            #x = torch.cat((x, dur, eng), 2)
            x = torch.cat((x, dur, eng), 2)

        # if the input dimension is different from the Transformer embedding dimension, project the input to same dim
        if self.frontend_dim != self.gop_dim:
            x = self.in_proj(x)

        if self.use_cano:
            canophn_emb = self.phn_embed(canophn.long()+1).float()
            canophn_embed = self.canophn_proj(canophn_emb)
            # NOTE: add phn embed
            x = x + canophn_embed

        # NOTE: add pos embed
        if self.use_pos:
            x = x + self.pos_embed

        # NOTE: bies emb
        if self.use_bies:
            bies_emb = self.bies_embed(bies.long()+1).float()
            bies_embed = self.bies_proj(bies_emb)
            x = x + bies_embed

        # block 1
        x = self.phn_block_1(x)
        # block 2
        x = self.phn_block_2(x)
        # block 3
        x = self.phn_block_3(x)

        # NOTE: phone recognition
        logits = self.phn_mlp_recog(x)

        # NOTE: phone score
        p = self.phn_mlp_score(x)

        x = self.wrd_block_4(x)

        if self.use_conv:
            word_conv = self.wrd_conv(x.transpose(1,2)).transpose(1,2)
        else:
            word_conv = x

        # NOTE: word score
        w1 = self.wrd_mlp_score_1(word_conv)
        w2 = self.mapping(self.wrd_mlp_score_2(word_conv))
        w3 = self.wrd_mlp_score_3(word_conv)

        # 25 x 24
        word_conv = self.utt_block_5(word_conv)

        if self.pool_mode == "attn":
            x = self.utt_pool(word_conv, word_conv, mask)
        elif self.pool_mode == "score-attn":
            s = torch.cat((p, w1, w2, w3), dim=-1)
            x = self.utt_pool(word_conv, s, mask)
        else:
            x = self.utt_pool(word_conv, mask)

		# NOTE: utterance score
        u1 = self.utt_mlp_score_1(x) # acc
        u2 = self.utt_mlp_score_2(x) # comp
        u3 = self.utt_mlp_score_3(x) # flu
        u4 = self.utt_mlp_score_4(x) # pro
        u5 = self.utt_mlp_score_5(x) # tot

        return u1, u2, u3, u4, u5, p, w1, w2, w3, logits

